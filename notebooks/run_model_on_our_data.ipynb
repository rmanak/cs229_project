{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32a9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fitz\n",
    "!pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32360f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141511a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Transcript PDF File\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip earning_call_validation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e099e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_call_train_path = \"Earning Call Transcript - Training Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0805c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb\t\t\t\t\t   glove.6B.zip\r\n",
      "aclImdb_v1.tar.gz\t\t\t   glove_lstm_model.keras\r\n",
      "baseline.ipynb\t\t\t\t   glove_vecs.bin\r\n",
      "earning_call_train.zip\t\t\t   glove_word_averaging_model.h5\r\n",
      "Earning Call Transcript - Training Data    glove_word_averaging_model.keras\r\n",
      "Earning Call Transcript - Validation Data  imdb_full_dataset.csv\r\n",
      "earning_call_validation.zip\t\t   infer_financial_sentiments.ipynb\r\n",
      "financial_data_kaggle.csv\t\t   run_model_on_our_data.ipynb\r\n",
      "financial_labels.csv\t\t\t   using_glove.ipynb\r\n",
      "glove.42B.300d.txt\t\t\t   vocab_index.json\r\n",
      "glove.42B.300d.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27319a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_call_validation_path = \"Earning Call Transcript - Validation Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359dba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = os.listdir(earning_call_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8953f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('financial_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503657a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pdf_to_label(pdf_filename, labels_df):\n",
    "    # Remove extension and any known formatting from the PDF filename\n",
    "    clean_pdf_filename = pdf_filename.lower().replace('.pdf', '').replace('_', ' ')\n",
    "    # Attempt to match the cleaned PDF filename with entries in the labels dataframe\n",
    "    for _, row in labels_df.iterrows():\n",
    "        if row['file_name'].lower() in clean_pdf_filename:\n",
    "            return row['label']\n",
    "    return None  # If no match is found, you might want to handle this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacdeb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store matched labels with PDF filenames\n",
    "matched_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591efd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over PDF files and match them to labels\n",
    "for pdf_file in pdf_files:\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        matched_label = match_pdf_to_label(pdf_file, labels_df)\n",
    "        matched_labels[pdf_file] = matched_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f045e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize and convert to lower case, initial tokenization for removing stop words, punctuation, and normalizing the text.\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped_tokens if word.isalpha()]\n",
    "    \n",
    "    # Filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transcript(text):\n",
    "    # Remove URLs in the footer\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Normalize text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove date and time patterns, e.g., \"2/7/24, 8:42 AM\"\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APM]{2}', '', text)\n",
    "\n",
    "    # Remove any text that looks like a website, e.g., \"seeking alpha\"\n",
    "    text = re.sub(r'seeking alpha', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove headers that might be repeated, e.g., earnings call transcript titles\n",
    "    text = re.sub(r'earnings call transcript', '', text)\n",
    "\n",
    "    # Remove numeric patterns that don't add value, e.g., \"Q4 2023\"\n",
    "    text = re.sub(r'q[1-4] \\d{4}', '', text)\n",
    "\n",
    "    # Remove specific phrases or patterns that don't contribute to the analysis\n",
    "    phrases_to_remove = [\n",
    "        r'\\bfeb\\. \\d{2}, 2024\\b',\n",
    "        r'\\b\\d{1,2}:\\d{2} [apm]{2} et\\b',\n",
    "        r'\\btranscripts\\b',\n",
    "        r'\\bfollowers\\b',\n",
    "        # Add more patterns as needed\n",
    "    ]\n",
    "    for phrase in phrases_to_remove:\n",
    "        text = re.sub(phrase, '', text)\n",
    "\n",
    "    # Remove any leading or trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Preprocess texts and store them\n",
    "preprocessed_texts = {}\n",
    "for pdf_file in pdf_files:\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(earning_call_train_path, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        # Preprocess the extracted text\n",
    "        clean_text = clean_transcript(text)\n",
    "        processed_text = preprocess_text(clean_text)\n",
    "        # Store the processed text with its matched label\n",
    "        matched_label = matched_labels.get(pdf_file)\n",
    "        preprocessed_texts[pdf_file] = (processed_text, matched_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed707a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4188de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb850e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
