{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46427bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0f74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773c56bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14149</td>\n",
       "      <td>I had two reasons for watching this swashbuckl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8946</td>\n",
       "      <td>This is, in my opinion, a very good film, espe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22378</td>\n",
       "      <td>I knew this film was supposed to be so bad it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12162</td>\n",
       "      <td>When the US entered World War I, the governmen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4879</td>\n",
       "      <td>Few movies can be viewed almost 60 years later...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0       14149  I had two reasons for watching this swashbuckl...      0\n",
       "1        8946  This is, in my opinion, a very good film, espe...      1\n",
       "2       22378  I knew this film was supposed to be so bad it ...      0\n",
       "3       12162  When the US entered World War I, the governmen...      1\n",
       "4        4879  Few movies can be viewed almost 60 years later...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you run the baseline notebook that prepares this data\n",
    "# from the original imdb dataset downloaded from the web.\n",
    "df = pd.read_csv(\"./imdb_full_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7487e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class one average 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"class one average\", np.mean(df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78cbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading glove vector from the web to local:\n",
    "# this may take a while\n",
    "# inside of this there is only 1 file\n",
    "# glove.42B.300d.txt\n",
    "# according to the website:https://nlp.stanford.edu/projects/glove/ \n",
    "# this one is uncased. \n",
    "\n",
    "# PLEASE MAKE SURE YOU UNCOMMENT AND RUN THE \n",
    "# COMMAND BELOW WHEN RUNNING THE NOTEBOOK FOR \n",
    "# THE FIRST TIME OR REST OF NOTEBOOK WON'T WORK\n",
    "\n",
    "# !wget https://nlp.stanford.edu/data/glove.42B.300d.zip ./glove.42B.300d.zip\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d85cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once downloaded unzip the file\n",
    "# !unzip glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f69a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also download other variations\n",
    "# I downloaded this one in particular seems like it\n",
    "# has various dimension sizes 50D, 100D etc...\n",
    "# inside of it there are 4 files:\n",
    "# glove.6B.100d.txt\n",
    "# glove.6B.200d.txt\n",
    "# glove.6B.300d.txt\n",
    "# glove.6B.50d.txt\n",
    "\n",
    "# WE MAY EXPERIMENT WITH SMALLER VECS LATER\n",
    "# FOR NOW WE DON'T NEED TO DOWNLOAD THIS ONE\n",
    "\n",
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip ./glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b997d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb\t\t   glove.42B.300d.txt  glove_vecs.bin\r\n",
      "aclImdb_v1.tar.gz  glove.42B.300d.zip  imdb_full_dataset.csv\r\n",
      "baseline.ipynb\t   glove.6B.zip        using_glove.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# check `glove.42B.300d.txt` and `imdb_full_dataset.csv` exists or see above comments \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7705f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading glove vectors into dictionary of word -> vec\n",
    "# this may take a while for first run, once run\n",
    "# it will cache to a local pickle file and will run faster\n",
    "\n",
    "def _get_glove_index_dict(unzipped_file=\"glove.42B.300d.txt\"):\n",
    "    _LOCAL_FILE = \"./glove_vecs.bin\"\n",
    "    local_file = Path(_LOCAL_FILE)\n",
    "    \n",
    "    if local_file.is_file():\n",
    "        with open(_LOCAL_FILE, \"rb\") as fp:\n",
    "            glove_index = pickle.load(fp)\n",
    "        \n",
    "        return glove_index\n",
    "    \n",
    "    glove_index = OrderedDict()\n",
    "    \n",
    "    with open(unzipped_file) as fp:\n",
    "        for line in fp.read().splitlines():\n",
    "            values = line.split(\" \")\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_index[word] = coefs\n",
    "    \n",
    "    with open(_LOCAL_FILE, 'wb') as fp:\n",
    "        pickle.dump(glove_index, fp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return glove_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c337aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_index = _get_glove_index_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5083eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num unique words in glove 1917494\n"
     ]
    }
   ],
   "source": [
    "print(\"num unique words in glove\", len(glove_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4def55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text: str) -> str:\n",
    "    s = input_text.lower()\n",
    "    s = s.replace('<br />', ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc955e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68059a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_and_create_embedding_matrix(corpus, glove_index):\n",
    "    \n",
    "    tok = tf.keras.preprocessing.text.Tokenizer(num_words=1000000, oov_token=\"<UNK>\")\n",
    "    tok.fit_on_texts(corpus)\n",
    "    \n",
    "    # for OOV tokens\n",
    "    num_words = len(tok.word_index) + 1\n",
    "    \n",
    "    # need to change this if other dim glove vecs are used\n",
    "    embedding_dim = 300 \n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, size=(num_words, embedding_dim))\n",
    "    \n",
    "    glove_oov = []\n",
    "    \n",
    "    for word, i in tok.word_index.items():\n",
    "        embedding_vector = glove_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # count the number of words not in embeddings_index\n",
    "            glove_oov.append(word)\n",
    "    \n",
    "    return embedding_matrix, tok, num_words, glove_oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b526740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, tok, num_words, glove_oov = _tokenize_and_create_embedding_matrix(df[\"text\"], glove_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0988470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20822 words from the corpus didn't exist in glove vecs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(glove_oov)} words from the corpus didn't exist in glove vecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a5f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', \"isn't\", \"wasn't\", \"couldn't\", '\\x96', \"they're\", \"won't\", \"wouldn't\", \"film's\", \"aren't\", \"haven't\", \"who's\", \"let's\", \"'the\", \"we're\", \"weren't\", \"movie's\", \"80's\", \"you'd\", \"hasn't\", \"man's\", \"shouldn't\", \"70's\", \"character's\", \"hadn't\", \"today's\", \"here's\", \"we've\", \"director's\", \"they've\", \"one's\", \"would've\", \"he'd\", \"ain't\", \"father's\", \"children's\", \"people's\", \"could've\", \"90's\", \"60's\", \"woman's\", \"they'd\", \"world's\", \"they'll\", \"50's\", \"show's\", \"1950's\", \"we'll\", \"he'll\", \"girl's\", \"mother's\", \"it'll\", \"hollywood's\", \"characters'\", \"someone's\", \"everyone's\", \"1970's\", \"disney's\", \"king's\", \"family's\"]\n"
     ]
    }
   ],
   "source": [
    "print(glove_oov[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "102b4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that seems strange... will get back to this\n",
    "# probably need to split the `'s'` or `'ll` etc...\n",
    "# these exist in glove vectors, confirming: \n",
    "# we should tokenize wouldn't to [\"would\", \"n't\"]\n",
    "# I think default keras tokenizer is no good.\n",
    "\n",
    "_, _, _ = glove_index[\"'s\"], glove_index[\"n't\"], glove_index[\"'re\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88efa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f653a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], \n",
    "    df[\"label\"], \n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31d43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tok.texts_to_sequences(text_train)\n",
    "X_test = tok.texts_to_sequences(text_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=512, padding=\"post\")\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=512, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af185e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 512, 300)          26575200  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512, 300)          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512, 16)           4816      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512, 16)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26580033 (101.39 MB)\n",
      "Trainable params: 26580033 (101.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=num_words, \n",
    "                     output_dim=300, \n",
    "                     input_length=512, \n",
    "                     weights=[embedding_matrix], \n",
    "                     trainable=True),\n",
    "    layers.Dropout(0.9),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1, activation=\"sigmoid\")]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a407d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e9520e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "40/40 [==============================] - 11s 238ms/step - loss: 0.6935 - binary_accuracy: 0.5273 - val_loss: 0.6861 - val_binary_accuracy: 0.4996\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 0.6786 - binary_accuracy: 0.6052 - val_loss: 0.6707 - val_binary_accuracy: 0.6510\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - 8s 208ms/step - loss: 0.6572 - binary_accuracy: 0.6668 - val_loss: 0.6403 - val_binary_accuracy: 0.7114\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - 8s 202ms/step - loss: 0.6238 - binary_accuracy: 0.7022 - val_loss: 0.5929 - val_binary_accuracy: 0.7600\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - 8s 192ms/step - loss: 0.5832 - binary_accuracy: 0.7337 - val_loss: 0.5497 - val_binary_accuracy: 0.7780\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - 8s 202ms/step - loss: 0.5429 - binary_accuracy: 0.7605 - val_loss: 0.5081 - val_binary_accuracy: 0.8056\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - 8s 192ms/step - loss: 0.5081 - binary_accuracy: 0.7865 - val_loss: 0.4715 - val_binary_accuracy: 0.8230\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - 8s 186ms/step - loss: 0.4722 - binary_accuracy: 0.8037 - val_loss: 0.4404 - val_binary_accuracy: 0.8370\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.4419 - binary_accuracy: 0.8217 - val_loss: 0.4133 - val_binary_accuracy: 0.8490\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.4151 - binary_accuracy: 0.8382 - val_loss: 0.3927 - val_binary_accuracy: 0.8540\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - 6s 147ms/step - loss: 0.3955 - binary_accuracy: 0.8438 - val_loss: 0.3755 - val_binary_accuracy: 0.8608\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - 6s 152ms/step - loss: 0.3783 - binary_accuracy: 0.8523 - val_loss: 0.3575 - val_binary_accuracy: 0.8678\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - 6s 148ms/step - loss: 0.3555 - binary_accuracy: 0.8618 - val_loss: 0.3448 - val_binary_accuracy: 0.8730\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - 5s 135ms/step - loss: 0.3478 - binary_accuracy: 0.8623 - val_loss: 0.3375 - val_binary_accuracy: 0.8736\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - 6s 148ms/step - loss: 0.3344 - binary_accuracy: 0.8696 - val_loss: 0.3275 - val_binary_accuracy: 0.8750\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - 6s 159ms/step - loss: 0.3173 - binary_accuracy: 0.8795 - val_loss: 0.3187 - val_binary_accuracy: 0.8800\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 0.3148 - binary_accuracy: 0.8805 - val_loss: 0.3135 - val_binary_accuracy: 0.8808\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - 6s 147ms/step - loss: 0.2947 - binary_accuracy: 0.8891 - val_loss: 0.3083 - val_binary_accuracy: 0.8816\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - 6s 142ms/step - loss: 0.2915 - binary_accuracy: 0.8875 - val_loss: 0.3021 - val_binary_accuracy: 0.8844\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - 6s 142ms/step - loss: 0.2828 - binary_accuracy: 0.8942 - val_loss: 0.3002 - val_binary_accuracy: 0.8836\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - 4s 113ms/step - loss: 0.2745 - binary_accuracy: 0.8946 - val_loss: 0.2951 - val_binary_accuracy: 0.8868\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 0.2667 - binary_accuracy: 0.8984 - val_loss: 0.2919 - val_binary_accuracy: 0.8878\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.2575 - binary_accuracy: 0.9036 - val_loss: 0.2890 - val_binary_accuracy: 0.8878\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 0.2524 - binary_accuracy: 0.9038 - val_loss: 0.2866 - val_binary_accuracy: 0.8888\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 0.2435 - binary_accuracy: 0.9101 - val_loss: 0.2845 - val_binary_accuracy: 0.8894\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 0.2384 - binary_accuracy: 0.9097 - val_loss: 0.2832 - val_binary_accuracy: 0.8914\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - 4s 85ms/step - loss: 0.2317 - binary_accuracy: 0.9129 - val_loss: 0.2820 - val_binary_accuracy: 0.8922\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 0.2302 - binary_accuracy: 0.9143 - val_loss: 0.2809 - val_binary_accuracy: 0.8928\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - 4s 113ms/step - loss: 0.2233 - binary_accuracy: 0.9173 - val_loss: 0.2783 - val_binary_accuracy: 0.8940\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.2220 - binary_accuracy: 0.9186 - val_loss: 0.2777 - val_binary_accuracy: 0.8932\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 0.2165 - binary_accuracy: 0.9191 - val_loss: 0.2756 - val_binary_accuracy: 0.8938\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - 5s 122ms/step - loss: 0.2106 - binary_accuracy: 0.9233 - val_loss: 0.2745 - val_binary_accuracy: 0.8934\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.2011 - binary_accuracy: 0.9229 - val_loss: 0.2735 - val_binary_accuracy: 0.8948\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.1990 - binary_accuracy: 0.9271 - val_loss: 0.2734 - val_binary_accuracy: 0.8956\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.1990 - binary_accuracy: 0.9283 - val_loss: 0.2722 - val_binary_accuracy: 0.8960\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 0.1912 - binary_accuracy: 0.9313 - val_loss: 0.2719 - val_binary_accuracy: 0.8958\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.1885 - binary_accuracy: 0.9315 - val_loss: 0.2723 - val_binary_accuracy: 0.8964\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 0.1841 - binary_accuracy: 0.9329 - val_loss: 0.2727 - val_binary_accuracy: 0.8972\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.1761 - binary_accuracy: 0.9351 - val_loss: 0.2713 - val_binary_accuracy: 0.8974\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - 3s 80ms/step - loss: 0.1778 - binary_accuracy: 0.9354 - val_loss: 0.2712 - val_binary_accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cd93ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tok.texts_to_sequences(text_train)\n",
    "X_test = tok.texts_to_sequences(text_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=128, padding=\"post\")\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=128, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e85f7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 128, 300)          26575200  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128, 300)          0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128, 64)           19264     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128, 64)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26628545 (101.58 MB)\n",
      "Trainable params: 26628545 (101.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=num_words, \n",
    "                     output_dim=300, \n",
    "                     input_length=128, \n",
    "                     weights=[embedding_matrix], \n",
    "                     trainable=True),\n",
    "    layers.Dropout(0.9),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1, activation=\"sigmoid\")]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c605c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08d2c39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "40/40 [==============================] - 13s 252ms/step - loss: 0.6935 - binary_accuracy: 0.5139 - val_loss: 0.6921 - val_binary_accuracy: 0.5342\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - 9s 212ms/step - loss: 0.6853 - binary_accuracy: 0.5479 - val_loss: 0.6865 - val_binary_accuracy: 0.5226\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - 8s 195ms/step - loss: 0.6442 - binary_accuracy: 0.6355 - val_loss: 0.5694 - val_binary_accuracy: 0.7310\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - 7s 177ms/step - loss: 0.6070 - binary_accuracy: 0.6848 - val_loss: 0.6111 - val_binary_accuracy: 0.6884\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - 7s 177ms/step - loss: 0.5626 - binary_accuracy: 0.7290 - val_loss: 0.6332 - val_binary_accuracy: 0.6878\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.5204 - binary_accuracy: 0.7560 - val_loss: 0.5267 - val_binary_accuracy: 0.7734\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.4885 - binary_accuracy: 0.7743 - val_loss: 0.4466 - val_binary_accuracy: 0.8068\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 0.4708 - binary_accuracy: 0.7836 - val_loss: 0.5281 - val_binary_accuracy: 0.7914\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 0.4315 - binary_accuracy: 0.8062 - val_loss: 0.5652 - val_binary_accuracy: 0.7870\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - 6s 136ms/step - loss: 0.4138 - binary_accuracy: 0.8187 - val_loss: 0.4899 - val_binary_accuracy: 0.8144\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.3893 - binary_accuracy: 0.8319 - val_loss: 0.4250 - val_binary_accuracy: 0.8234\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - 4s 113ms/step - loss: 0.3809 - binary_accuracy: 0.8357 - val_loss: 0.5352 - val_binary_accuracy: 0.8078\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - 4s 84ms/step - loss: 0.3678 - binary_accuracy: 0.8421 - val_loss: 0.4413 - val_binary_accuracy: 0.8330\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - 3s 79ms/step - loss: 0.3512 - binary_accuracy: 0.8494 - val_loss: 0.4431 - val_binary_accuracy: 0.8346\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.3490 - binary_accuracy: 0.8503 - val_loss: 0.4671 - val_binary_accuracy: 0.8230\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.3222 - binary_accuracy: 0.8676 - val_loss: 0.4707 - val_binary_accuracy: 0.8362\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.3218 - binary_accuracy: 0.8638 - val_loss: 0.4023 - val_binary_accuracy: 0.8524\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3096 - binary_accuracy: 0.8729 - val_loss: 0.3992 - val_binary_accuracy: 0.8470\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - 3s 62ms/step - loss: 0.3081 - binary_accuracy: 0.8746 - val_loss: 0.3808 - val_binary_accuracy: 0.8536\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2905 - binary_accuracy: 0.8802 - val_loss: 0.3663 - val_binary_accuracy: 0.8576\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.2872 - binary_accuracy: 0.8839 - val_loss: 0.4082 - val_binary_accuracy: 0.8568\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.2768 - binary_accuracy: 0.8845 - val_loss: 0.4246 - val_binary_accuracy: 0.8496\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.2685 - binary_accuracy: 0.8923 - val_loss: 0.3898 - val_binary_accuracy: 0.8636\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.2571 - binary_accuracy: 0.8948 - val_loss: 0.3565 - val_binary_accuracy: 0.8720\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 0.2606 - binary_accuracy: 0.8943 - val_loss: 0.4110 - val_binary_accuracy: 0.8646\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2500 - binary_accuracy: 0.8989 - val_loss: 0.4074 - val_binary_accuracy: 0.8660\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2396 - binary_accuracy: 0.9047 - val_loss: 0.4269 - val_binary_accuracy: 0.8572\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.2363 - binary_accuracy: 0.9055 - val_loss: 0.4131 - val_binary_accuracy: 0.8488\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2334 - binary_accuracy: 0.9068 - val_loss: 0.3861 - val_binary_accuracy: 0.8746\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 0.2242 - binary_accuracy: 0.9126 - val_loss: 0.4446 - val_binary_accuracy: 0.8598\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.2213 - binary_accuracy: 0.9111 - val_loss: 0.4862 - val_binary_accuracy: 0.8546\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 0.2263 - binary_accuracy: 0.9099 - val_loss: 0.3981 - val_binary_accuracy: 0.8798\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.2075 - binary_accuracy: 0.9204 - val_loss: 0.4112 - val_binary_accuracy: 0.8720\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 0.2053 - binary_accuracy: 0.9195 - val_loss: 0.4335 - val_binary_accuracy: 0.8774\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 0.2041 - binary_accuracy: 0.9204 - val_loss: 0.4024 - val_binary_accuracy: 0.8750\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.1906 - binary_accuracy: 0.9259 - val_loss: 0.3916 - val_binary_accuracy: 0.8768\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - 1s 27ms/step - loss: 0.1882 - binary_accuracy: 0.9276 - val_loss: 0.4603 - val_binary_accuracy: 0.8702\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.1844 - binary_accuracy: 0.9309 - val_loss: 0.4127 - val_binary_accuracy: 0.8766\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.1838 - binary_accuracy: 0.9282 - val_loss: 0.4597 - val_binary_accuracy: 0.8682\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.1732 - binary_accuracy: 0.9348 - val_loss: 0.4088 - val_binary_accuracy: 0.8766\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cdc74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
